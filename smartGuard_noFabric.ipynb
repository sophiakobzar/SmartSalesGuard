{"cells":[{"cell_type":"markdown","id":"7e6bf706-a3bb-4439-974c-33b78b2ba490","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["# Track Machine Learning experiments and models\n","\n","##### This notebook demonstrates an anomaly detection process on a sales dataset using Spark, Pandas, and Isolation Forest. The steps include:\n","###### 1. Initializing a Spark session and enabling Arrow optimization for efficient data transfer between Spark and Pandas.\n","###### 2. Loading the entire sales dataset from the lakehouse into a Spark DataFrame.\n","###### 3. Converting the Spark DataFrame to a Pandas DataFrame for further processing.\n","###### 4. Cleaning the 'Sales' column by removing non-numeric characters and converting 'Sales' and 'Profit' columns to numeric types.\n","###### 5. Dropping rows with NaN values in the 'Sales' and 'Profit' columns.\n","###### 6. Normalizing the 'Sales' and 'Profit' data using StandardScaler.\n","###### 7. Initializing and fitting an Isolation Forest model to detect anomalies in the normalized data.\n","###### 8. Predicting anomalies and classifying data points as normal or anomalous.\n","###### 9. Visualizing the anomalies using a scatter plot with improved readability and aesthetics.\n"]},{"cell_type":"code","execution_count":4,"id":"1428aa58-a03a-446f-a07b-38acd68db128","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of entries after removing anomalies: (5827, 20)\n"]}],"source":["import pandas as pd\n","import re\n","from sklearn.ensemble import IsolationForest\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load data into pandas DataFrame from CSV file\n","pandas_df = pd.read_csv(\"SuperStore Sales DataSet.csv\")\n","\n","# Clean the 'Sales' column by removing non-numeric characters\n","pandas_df['Sales'] = pandas_df['Sales'].apply(lambda x: re.sub(r'[^0-9.]', '', str(x)))\n","\n","# Convert 'Sales' and 'Profit' columns to numeric\n","pandas_df['Sales'] = pd.to_numeric(pandas_df['Sales'], errors='coerce')\n","pandas_df['Profit'] = pd.to_numeric(pandas_df['Profit'], errors='coerce')\n","\n","# Drop rows with NaN values\n","pandas_df.dropna(subset=['Sales', 'Profit'], inplace=True)\n","\n","# Normalize the data\n","scaler = StandardScaler()\n","pandas_df[['Sales', 'Profit']] = scaler.fit_transform(pandas_df[['Sales', 'Profit']])\n","\n","# Initialize the Isolation Forest model\n","iso_forest = IsolationForest(contamination=0.0125, random_state=42)\n","\n","# Fit the model\n","iso_forest.fit(pandas_df[['Sales', 'Profit']].values)\n","\n","# Predict anomalies using the same feature names\n","pandas_df['anomaly'] = iso_forest.predict(pandas_df[['Sales', 'Profit']].values)\n","\n","# Remove anomalies\n","cleaned_df = pandas_df[pandas_df['anomaly'] == 1]\n","\n","# Drop the 'anomaly' column as it's no longer needed\n","cleaned_df = cleaned_df.drop(columns=['anomaly'])\n","\n","# Save the cleaned data back to a CSV file\n","cleaned_df.to_csv(\"Cleaned_SuperStore_Sales_DataSet.csv\", index=False)\n","\n","print(\"Number of entries after removing anomalies:\", cleaned_df.shape)"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"bb32fd5e-b0e2-43ed-aff6-a5a38d55f625","default_lakehouse_name":"lakehouseTraining","default_lakehouse_workspace_id":"dc2b9547-cb65-4419-b6da-db854bb4bbff"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.0"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
